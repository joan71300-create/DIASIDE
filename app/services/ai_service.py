import google.genai
from google.genai import types # Add this import
import os
import asyncio
import json
from opik.integrations.genai import track_genai
from opik import track
from app.core.config import settings
from app.models import schemas
from app.core.guardrails import SafetyGuardrails
from app.core.prompts import COACH_SYSTEM_PROMPT

class AIService:
    def __init__(self):
        # Workaround: google-genai peut privil√©gier GOOGLE_API_KEY si pr√©sent
        if "GOOGLE_API_KEY" in os.environ:
            del os.environ["GOOGLE_API_KEY"]
            
        # Initialisation du client Gemini
        self.client = google.genai.Client(api_key=settings.GEMINI_API_KEY)
        # Note: track_genai wrapping might need adjustment for async calls or handled differently.
        # For now, using raw client for async support to ensure timeout works.
        self.gemini_client = track_genai(self.client, project_name="DIASIDE")

    @track(name="generate_coach_advice")
    async def generate_coach_advice(self, user_results: dict, history: list = [], user_message: str = None, health_context: str = "", image_bytes: bytes = None) -> dict:
        """
        Ticket B06/DS-B-011: Consultation Gemini 3.0 avec injection dynamique et r√©ponse structur√©e (JSON).
        G√®re le timeout (10s) et les rate limits.
        
        Ticket AI-001: Ajout du support Multi-Turn (history + user_message).
        Ticket AI-005: Utilisation du prompt centralis√© avec Few-Shot.
        """
        try:
            # Injection dynamique du JSON user_results
            prompt = f"{COACH_SYSTEM_PROMPT}\n\n"
            
            if health_context:
                prompt += f"{health_context}\n\n"
            
            prompt += f"Analyse Stabilit√© & M√©dicale :\n{user_results}"
            
            # --- TICKET AI-001: Context Injection ---
            if history:
                prompt += "\n\nHistorique de la conversation (derniers messages) :\n"
                # Sliding window simple (last 5 messages) to save tokens
                for msg in history[-5:]:
                    # Handle both dict and Pydantic model safely
                    if isinstance(msg, dict):
                        role = msg.get('role', 'user')
                        content = msg.get('content', '')
                    else:
                        role = getattr(msg, 'role', 'user')
                        content = getattr(msg, 'content', '')
                        
                    prompt += f"- {role.upper()}: {content}\n"
            
            if user_message:
                prompt += f"\n\nNouvelle question de l'utilisateur : {user_message}"
            
            # Si image pr√©sente, on l'ajoute au prompt comme "Regarde √ßa"
            if image_bytes:
                prompt += "\n\n[IMAGE INCLUSE] L'utilisateur a joint une image (Graphique ou Repas) pour analyse."
            # ----------------------------------------

            print(f"--- Envoi √† Gemini 2.5 Flash (Timeout 20s): {prompt[:200]}... ---")
            
            # Construction du contenu (Texte + Image potentielle)
            contents = [prompt]
            if image_bytes:
                image_part = types.Part.from_bytes(
                    data=image_bytes,
                    mime_type="image/jpeg" # On assume JPEG pour simplifier, ou on d√©tectera plus tard
                )
                contents.append(image_part)

            # Utilisation de asyncio.wait_for pour le timeout
            # Utilisation de client.aio pour l'appel asynchrone
            response = await asyncio.wait_for(
                self.client.aio.models.generate_content(
                    model="gemini-2.5-flash", 
                    contents=contents,
                    config={'response_mime_type': 'application/json'}
                ),
                timeout=20.0
            )
            response_text = response.text
            
            # Parsing JSON
            try:
                response_json = json.loads(response_text)
                advice_text = response_json.get("advice", "")
            except json.JSONDecodeError:
                # Fallback si le mod√®le renvoie du texte brut
                response_json = {"advice": response_text, "actions": []}
                advice_text = response_text

            # --- TICKET B07: GUARDRAILS ---
            # On v√©rifie le texte du conseil
            is_safe_keyword, reason_keyword = SafetyGuardrails.check_keywords(advice_text)
            if not is_safe_keyword:
                print(f"üö´ BLOCKED by Regex: {reason_keyword}")
                raise ValueError(f"Safety Violation: {reason_keyword}")

            # 2. LLM-as-a-Judge (Opik Scorer)
            judge_prompt = SafetyGuardrails.get_judge_prompt(advice_text)
            try:
                # Appel rapide au juge (timeout court 5s)
                judge_response = await asyncio.wait_for(
                    self.client.aio.models.generate_content(
                        model="gemini-2.5-flash",
                        contents=judge_prompt,
                        config={'response_mime_type': 'application/json'}
                    ),
                    timeout=5.0
                )
                evaluation = json.loads(judge_response.text)
                
                # Log to Opik
                print(f"‚öñÔ∏è LLM Judge Score: {evaluation}")
                
                if not evaluation.get("safe", True):
                    print(f"üö´ BLOCKED by LLM Judge: {evaluation.get('reason')}")
                    raise ValueError(f"Safety Violation: {evaluation.get('reason')}")
                    
            except Exception as e_judge:
                print(f"‚ö†Ô∏è LLM Judge Error: {e_judge}")

            return response_json

        except ValueError as ve:
            raise ve
        except asyncio.TimeoutError:
            print("‚ùå Timeout Gemini (20s exceeded)")
            return {"advice": "D√©sol√©, le service est un peu lent. Veuillez r√©essayer.", "actions": []}
        except Exception as e:
            print(f"‚ùå Erreur Gemini : {e}")
            if "429" in str(e) or "503" in str(e):
                return {"advice": "Le service est temporairement surcharg√©. Veuillez r√©essayer.", "actions": []}
            return {"advice": f"Erreur IA: {str(e)}", "actions": []}

    def format_health_context(self, snapshot: schemas.UserHealthSnapshot) -> str:
        """
        Transforme un UserHealthSnapshot en contexte textuel pour le prompt Gemini.
        Traceable via Opik car utilis√© dans le flux IA.
        """
        # Format Activity History
        activity_context = "Pas d'activit√© r√©cente enregistr√©e."
        if snapshot.recent_activity:
            last_stats = snapshot.recent_activity[-1] # Most recent
            activity_context = (
                f"Derni√®re activit√© ({last_stats.date.strftime('%Y-%m-%d')}): {last_stats.steps} pas, "
                f"{last_stats.calories_burned} kcal br√ªl√©es."
            )

        # Format Meal History
        meal_context = "Pas de repas r√©cents enregistr√©s."
        if snapshot.recent_meals:
            recent_meals_str = [f"- {m.timestamp.strftime('%H:%M')}: {m.name} ({m.carbs}g glucides)" for m in snapshot.recent_meals[-3:]]
            meal_context = "Derniers repas :\n" + "\n".join(recent_meals_str)

        return (
            f"PROFIL PATIENT :\n"
            f"- Info: {snapshot.age} ans, {snapshot.lifestyle.gender}, {snapshot.diabetes_type}\n"
            f"- Biom√©trie: {snapshot.weight}kg, {snapshot.height}cm\n"
            f"- Objectif Pas: {snapshot.lifestyle.daily_step_goal}/jour\n"
            f"- Activit√©: Niveau {snapshot.lifestyle.activity_level.value}. {activity_context}\n"
            f"- Nutrition: R√©gime {snapshot.lifestyle.diet_type}. {meal_context}\n"
            f"- Labo: HbA1c {snapshot.lab_data.hba1c}%, Glyc√©mie √† jeun {snapshot.lab_data.fasting_glucose}mg/dL.\n"
        )

ai_service = AIService()
